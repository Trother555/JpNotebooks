{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we compare 2 simple text classifiers. One is SVC and another is a neural network with few layers. We use NLTK and wordnet database for tokenization and lemmatization and sklearn's TfidfVectorizer and gensim's word2vec to vectorize words for models.\n",
    "First we're going to load some labeled texts. Labels are 0 or 1 for negative or positive sentiment they express respectively. Then we'll brake texts into separate words, lemmatize and exclude stop-words. Next we'll train our models. SVC will use tfid vectorization, while neural natwork will deal with word2vec text representation model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, sep=\"\\t\", header=None, train_ratio = 0.7):\n",
    "    \"\"\"Loads labeled dataset and returns (train, test) pd dataframe tuple \n",
    "    \"\"\"\n",
    "    d = pd.read_csv(path,sep=sep,header=header)\n",
    "    train_count = round(train_ratio*d.shape[0])\n",
    "    d_train = d[:train_count]\n",
    "    d_test = d[train_count:d.shape[0]]\n",
    "    return (d_train, d_test.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(pos_tag):\n",
    "    \"\"\"Converts POS returned by nltk.pos_tag to wordnet's POS\n",
    "    \"\"\"\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        #Noun is a default case for WordNetLemmatizer\n",
    "        #so if we can't tell the lemma, lemmatizer consider it as a noun\n",
    "        #thus it's correct to return noun by default\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemma_by_pos(tagged_word):\n",
    "    \"\"\"Get word's lemma by its pos-tag\n",
    "    For instance: lemma_by_pos((\"going\",\"v\")) returns \"go\"\n",
    "    \"\"\"\n",
    "    return lm.lemmatize(tagged_word[0],get_wordnet_pos(tagged_word[1]))\n",
    "\n",
    "def tokenize_and_lemmatize(texts):\n",
    "    \"\"\"Tokenizes and then lemmatizes list of texts into list of lists of word lemmas\n",
    "    \"\"\"\n",
    "    #Get a list of pos-tagged and tokenized text\n",
    "    tag_tok = [nltk.pos_tag(word_tokenize(text)) for text in texts]\n",
    "    return [[lemma_by_pos(tok) for tok in tok_list] for tok_list in tag_tok]\n",
    "\n",
    "#Supposingly, these words are not usefull for classification so they are to be excluded from texts\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['.', ';', '!',',','-','&','...','?',':',')','(',\"'s\",\"n't\",\"''\",\"``\",\"'\"]+[i for i in \"1234567890\"])\n",
    "def prepare_data(texts):\n",
    "    \"\"\"Lematize the list of documents and drop stop words from it.\n",
    "    Returns list of list of tokens\n",
    "    \"\"\"\n",
    "    tokenized = tokenize_and_lemmatize(texts)\n",
    "    return [[token for token in text if (not token in stop_words)]for text in tokenized]\n",
    "\n",
    "#This is for SVC classifier as it uses TfidfVectorizer which needs raw text lists\n",
    "def prepare_data_raw(texts):\n",
    "    \"\"\"Lematize the list of documents and drop stop words from it.\n",
    "    Returns list of string texts\n",
    "    \"\"\"\n",
    "    return [\" \".join(text) for text in prepare_data(texts)]\n",
    "\n",
    "#This is for NN classifier, which is using (n,100,300) tensors in which each of n sample texts\n",
    "#is being represented as a seqence of 300-dimension vectors (each word is a vector) and padded with zero vectors\n",
    "#to have length of 100. This illustrates what was said above:\n",
    "#\n",
    "#       |sample_1| \n",
    "#       |sample_2|\n",
    "#data = |...     |, where sample_i = (word_1, word_2, ..., word_100), \n",
    "#       |...     |   where word_i = (w1, w2, ..., w300),\n",
    "#       |sample_n|   where w_i is a float number.\n",
    "#\n",
    "def prepare_data_v2w(texts, wvmodel):\n",
    "    \"\"\"Lemmatize the list of documents and drop stop words from it.\n",
    "    Returns (n, 100, 300) tensor for built NN\n",
    "    \"\"\"\n",
    "    vectors = [[wvmodel.wv[word] for word in text] for text in prepare_data(texts)]\n",
    "    return sequence.pad_sequences(vectors, maxlen=20,value=[np.array(range(100))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC\n",
    "from sklearn import datasets, grid_search\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def classify_SVC(data, target, t_data, t_target):\n",
    "    \"\"\"Returns SVC model and its score on given data\n",
    "    \"\"\"\n",
    "    #Note! We use both test and train data to construct a dictionary\n",
    "    prepared_dictionary = prepare_data(np.concatenate([data,test_data]))\n",
    "    prepared_data = prepare_data(data)\n",
    "    #Find tfid representation among all documents\n",
    "    raw = \" \".join([word for text in prepared_dictionary for word in text])\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit_transform([raw])\n",
    "    #Find the best C parametr for SVC classifier using grid search\n",
    "    grid = {'C': np.power(10.0, np.arange(-5, 6))}\n",
    "    cv = KFold(target.size, n_folds=5, shuffle=True, random_state=241)\n",
    "    model = SVC(kernel='linear', random_state=241)\n",
    "    gs = grid_search.GridSearchCV(model, grid, scoring='accuracy', cv=cv)\n",
    "    gs.fit(vectorizer.transform(prepare_data_raw(data)), target)\n",
    "    best_param = max(gs.grid_scores_,key=lambda x: x.mean_validation_score)\n",
    "    #Teach SVC with the best C\n",
    "    model = SVC(kernel='linear', random_state=241, C=best_param[0]['C'])\n",
    "    model.fit(vectorizer.transform(prepare_data_raw(data)), target)\n",
    "    return (model, model.score(vectorizer.transform(prepare_data_raw(t_data)),t_target))\n",
    "#NN\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, GRU, Dropout, BatchNormalization\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models import word2vec\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def classify_NN(data, target, t_data, t_target):\n",
    "    #Note! We use both test and train data to construct a dictionary\n",
    "    prepared_dictionary = prepare_data(np.concatenate([data,test_data]))\n",
    "    prepared_data = prepare_data(data)\n",
    "    #Build the word to vec model\n",
    "    wvmodel = word2vec.Word2Vec(prepared_dictionary,iter=20,min_count = 1,size=100,workers=4)\n",
    "    #Build NN\n",
    "    input = Input(shape=(None, 100))\n",
    "    #out = GRU(100, return_sequences=True)(input)\n",
    "    out = Dense(64)(input)\n",
    "    #out = Dense(256)(out)\n",
    "    out = LSTM(32)(out)\n",
    "    out = Dense(2, activation='softmax')(out)\n",
    "    model = Model(input, out)\n",
    "    model.compile(\"adam\", loss='categorical_crossentropy')\n",
    "    #Converts a class vector (integers) to binary class matrix (because the loss is categorical_crossentropy)\n",
    "    target = to_categorical(target)\n",
    "    #Train model\n",
    "    model.fit(prepare_data_v2w(data, wvmodel),target,epochs=4)\n",
    "    #Test and return score and the model\n",
    "    return (model, model.test_on_batch(prepare_data_v2w(test_data, wvmodel),to_categorical(test_target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the data try out classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = load_dataset(\"yelp_labelled.txt\")\n",
    "target = train[1]\n",
    "data = train[0]\n",
    "test_target = test[1]\n",
    "test_data = test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC score: 0.71\n"
     ]
    }
   ],
   "source": [
    "print(f\"SVC score: {classify_SVC(data,target,test_data,test_target)[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "700/700 [==============================] - 7s 9ms/step - loss: 0.6837\n",
      "Epoch 2/4\n",
      "700/700 [==============================] - 0s 356us/step - loss: 0.6817\n",
      "Epoch 3/4\n",
      "700/700 [==============================] - 0s 357us/step - loss: 0.6826\n",
      "Epoch 4/4\n",
      "700/700 [==============================] - 0s 372us/step - loss: 0.6792\n",
      "NN score: 0.7270189523696899\n"
     ]
    }
   ],
   "source": [
    "print(f\"NN score: {classify_NN(data,target,test_data,test_target)[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
